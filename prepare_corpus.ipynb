{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare a corpus for topic modeling<a id='top'></a>\n",
    "\n",
    "0. Create a new project directory in the [projects subfolder](projects). It should have the following structure:\n",
    "    * *projects/your_project_name/raw* for your raw corpus (e.g. as XML files or plain text)\n",
    "    * *projects/your_project_name/corpus* for your prepared corpus (will be filled by this notebook)\n",
    "    * *projects/your_project_name/results* for the results of the topic modeling process (will be filled by [create_lda.ipynb](create_lda.ipynb))\n",
    "    \n",
    "1. Put your raw corpus files into the corresponding filder  \n",
    "2. [Create corpus from XML files](#prepare_xml) or [create corpus from plain text files](#prepare_txt). This notebook demonstrates how to load texts, extract metadata and filter unwanted (German) POS (e.g. only nouns are left). The result is then saved as a json which can be used in the subsequent notebooks. \n",
    "3. [Create LDA](create_lda.ipynb) (different notebook)\n",
    "4. [Explore corpus](corpus.ipynb) (different notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global paths for your project\n",
    "your_project_name = \"zeit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global paths for corpus etc.\n",
    "raw_path = os.path.join(\"projects\", your_project_name, \"raw\")\n",
    "corpus_path = os.path.join(\"projects\", your_project_name, \"corpus\")\n",
    "result_path =os.path.join(\"projects\", your_project_name, \"results\")\n",
    "model_name = \"model\"\n",
    "topics_name = \"topics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare corpus from XML<a id='prepare_xml'></a>\n",
    "\n",
    "This cell demonstrates how to load a German TEI xml, extract metadata and texts and filter unwanted POS\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_only = \"ADJ\"\n",
    "keep_only = \"NOUN\"\n",
    "\n",
    "import spacy\n",
    "!{sys.executable} -m spacy download de_core_news_sm\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "for xml_file in tqdm(sorted(os.listdir(raw_path))):\n",
    "    output_json = []\n",
    "    print(xml_file)\n",
    "    if xml_file.endswith(\".xml\"):\n",
    "        # get TEI xml data\n",
    "        tree = ET.parse(os.path.join(raw_path, xml_file))\n",
    "        root = tree.getroot()\n",
    "        text = []\n",
    "        for text_node in root.findall(\".//{*}text\"):\n",
    "            entry = {}\n",
    "            entry[\"title\"] = text_node.get(\"title\")\n",
    "            entry[\"url\"] = xml_file\n",
    "            entry[\"date\"] = text_node.get(\"year\")\n",
    "            entry[\"author\"] = text_node.get(\"author\")\n",
    "            entry[\"comment_count\"] = 0\n",
    "            entry[\"text\"] = []\n",
    "            for txt in text_node:\n",
    "                # POS filtering\n",
    "                if txt.text is not None and len(txt.text.split())> 3:\n",
    "                    doc = nlp(txt.text)\n",
    "                    for w in doc:\n",
    "                        if w.pos_ == keep_only:\n",
    "                            entry[\"text\"].append(w.orth_)\n",
    "            output_json.append(entry)\n",
    "\n",
    "    with open(os.path.join(corpus_path, xml_file.split(\".\")[0] + \".json\"), 'w') as outfile:\n",
    "        json.dump(output_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare corpus from txt<a id='prepare_txt'></a>\n",
    "\n",
    "This cell demonstrates how to load a German plain text file, extract metadata from the file name and filter unwanted POS\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_only = \"ADJ\"\n",
    "keep_only = \"NOUN\"\n",
    "\n",
    "import spacy\n",
    "!{sys.executable} -m spacy download de_core_news_sm\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# increase max length for texts\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "for folder in tqdm(sorted(os.listdir(raw_path))):  \n",
    "    output_json = []\n",
    "    for txt_file in sorted(os.listdir(os.path.join(raw_path, folder))):\n",
    "        entry = {}\n",
    "        entry[\"title\"] = re.sub(\".txt\", \"\", txt_file.split(\"-\")[1])\n",
    "        entry[\"url\"] = os.path.join(folder, txt_file)\n",
    "        entry[\"date\"] = re.sub(\".txt\", \"\", txt_file.split(\"-\")[0]) + \" 00:00:00\"\n",
    "        entry[\"author\"] = \"\"\n",
    "        entry[\"comment_count\"] = 0\n",
    "        entry[\"text\"] = []\n",
    "        text = \"\"\n",
    "        with open(os.path.join(raw_path, folder, txt_file), \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # POS filtering\n",
    "        if text is not None and len(text.split())> 3:\n",
    "            doc = nlp(text)\n",
    "            for w in doc:\n",
    "                if w.pos_ == keep_only:\n",
    "                    entry[\"text\"].append(w.orth_)\n",
    "        output_json.append(entry)\n",
    "    \n",
    "    with open(os.path.join(corpus_path, folder + \".json\"), 'w') as outfile:\n",
    "         json.dump(output_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
