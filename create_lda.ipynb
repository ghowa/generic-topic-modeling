{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LDA for Corpus<a id='top'></a>\n",
    "\n",
    "0. Download an available corpus or create a new one. For the latter, create a JSON file for each blog or social media profile of your corpus; each entry or post is a line in a JSON file. One way to do this is to crawl websites using [scrapy](https://scrapy.org) with these flags: \"-o result.json -t json\" (see [sample crawlers](./scripts/scraper/spiders) and [example item file](./scripts/scraper/items.py)). An example JSON file is [here](./scripts/example.json).\n",
    "1. [Prepare corpus for the LDA](#prepare). This notebook demonstrates how to load a (German) TEI xml, extract metadata and texts and filter unwanted POS (only nouns are left). The result is then saved as a json which can be used in the subsequent cells. You can also prepare your corpus externally, see my [example](./scripts/text.py) which is tailored to Russian texts. It removes all non-cyrillic characters, removes all words which are not nouns and sets all nouns into first person singular using POS tagging. The result is again saved in a json file\n",
    "2. [Create LDA model for the corpus](#create)\n",
    "3. [Compute topic distribution for corpus](#compute)\n",
    "4. [Explore corpus](corpus.ipynb) (different notebook)\n",
    "\n",
    "Due to copyright reasons I cannot publish the scraped raw data. The results of the smoothing process in step 2 are [here](./corpus/); they are used in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from gensim import corpora, models\n",
    "import logging\n",
    "import errno\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import pytz\n",
    "import numpy as np\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# set global paths for corpus etc.\n",
    "corpus_path = \"alpenwort\"\n",
    "result_path = \"results_alpenwort_adj\"\n",
    "model_name = \"model\"\n",
    "topics_name = \"topics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare corpus<a id='prepare'></a>\n",
    "\n",
    "This cell demonstrates how to load a German TEI xml, extract metadata and texts and filter unwanted POS\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Users\\dr.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "100%|██████████| 92/92 [24:47<00:00, 16.16s/it]\n"
     ]
    }
   ],
   "source": [
    "keep_only = \"ADJ\"\n",
    "# keep_only = \"NOUN\"\n",
    "\n",
    "import spacy\n",
    "!{sys.executable} -m spacy download de_core_news_sm\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "for xml_file in tqdm(sorted(os.listdir(corpus_path))):\n",
    "    output_json = []\n",
    "    if xml_file.endswith(\".xml\"):\n",
    "        # get TEI xml data\n",
    "        tree = ET.parse(os.path.join(corpus_path, xml_file))\n",
    "        root = tree.getroot()\n",
    "        text = []\n",
    "        for text_node in root.findall(\".//{*}text\"):\n",
    "            entry = {}\n",
    "            entry[\"title\"] = text_node.get(\"title\")\n",
    "            entry[\"url\"] = xml_file\n",
    "            entry[\"date\"] = text_node.get(\"year\")\n",
    "            entry[\"author\"] = text_node.get(\"author\")\n",
    "            entry[\"comment_count\"] = 0\n",
    "            entry[\"text\"] = []\n",
    "            for txt in text_node:\n",
    "                # POS filtering\n",
    "                if txt.text is not None and len(txt.text.split())> 3:\n",
    "                    doc = nlp(txt.text)\n",
    "                    for w in doc:\n",
    "                        if w.pos_ == keep_only:\n",
    "                            entry[\"text\"].append(w.orth_)\n",
    "            output_json.append(entry)\n",
    "\n",
    "    with open(os.path.join(corpus_path, xml_file.split(\".\")[0] + \".json\"), 'w') as outfile:\n",
    "        json.dump(output_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LDA model for corpus<a id='create'></a>\n",
    "\n",
    "This cell creates the topic model for the specified corpus stored in JSON files\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:47:16,642 : INFO : initializing cython corpus reader from results_alpenwort_adj\\model.corp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  av_1900_031_TEI.json\n",
      "File:  av_1900_031_TEI.xml\n",
      "File:  av_1901_032_TEI.json\n",
      "File:  av_1901_032_TEI.xml\n",
      "File:  av_1902_033_TEI.json\n",
      "File:  av_1902_033_TEI.xml\n",
      "File:  av_1903_034_TEI.json\n",
      "File:  av_1903_034_TEI.xml\n",
      "File:  av_1904_035_TEI.json\n",
      "File:  av_1904_035_TEI.xml\n",
      "File:  av_1905_036_TEI.json\n",
      "File:  av_1905_036_TEI.xml\n",
      "File:  av_1906_037_TEI.json\n",
      "File:  av_1906_037_TEI.xml\n",
      "File:  av_1907_038_TEI.json\n",
      "File:  av_1907_038_TEI.xml\n",
      "File:  av_1908_039_TEI.json\n",
      "File:  av_1908_039_TEI.xml\n",
      "File:  av_1909_040_TEI.json\n",
      "File:  av_1909_040_TEI.xml\n",
      "File:  av_1910_041_TEI.json\n",
      "File:  av_1910_041_TEI.xml\n",
      "File:  av_1911_042_TEI.json\n",
      "File:  av_1911_042_TEI.xml\n",
      "File:  av_1912_043_TEI.json\n",
      "File:  av_1912_043_TEI.xml\n",
      "File:  av_1913_044_TEI.json\n",
      "File:  av_1913_044_TEI.xml\n",
      "File:  av_1914_045_TEI.json\n",
      "File:  av_1914_045_TEI.xml\n",
      "File:  av_1915_046_TEI.json\n",
      "File:  av_1915_046_TEI.xml\n",
      "File:  av_1916_047_TEI.json\n",
      "File:  av_1916_047_TEI.xml\n",
      "File:  av_1917_048_TEI.json\n",
      "File:  av_1917_048_TEI.xml\n",
      "File:  av_1918_049_TEI.json\n",
      "File:  av_1918_049_TEI.xml\n",
      "File:  av_1919_050_TEI.json\n",
      "File:  av_1919_050_TEI.xml\n",
      "File:  av_1920_051_TEI.json\n",
      "File:  av_1920_051_TEI.xml\n",
      "File:  av_1921_052_TEI.json\n",
      "File:  av_1921_052_TEI.xml\n",
      "File:  av_1922_053_TEI.json\n",
      "File:  av_1922_053_TEI.xml\n",
      "File:  av_1923_054_TEI.json\n",
      "File:  av_1923_054_TEI.xml\n",
      "File:  av_1924_055_TEI.json\n",
      "File:  av_1924_055_TEI.xml\n",
      "File:  av_1925_056_TEI.json\n",
      "File:  av_1925_056_TEI.xml\n",
      "File:  av_1926_057_TEI.json\n",
      "File:  av_1926_057_TEI.xml\n",
      "File:  av_1927_058_TEI.json\n",
      "File:  av_1927_058_TEI.xml\n",
      "File:  av_1928_059_TEI.json\n",
      "File:  av_1928_059_TEI.xml\n",
      "File:  av_1929_060_TEI.json\n",
      "File: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:47:17,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " av_1929_060_TEI.xml\n",
      "File:  av_1930_061_TEI.json\n",
      "File:  av_1930_061_TEI.xml\n",
      "File:  av_1931_062_TEI.json\n",
      "File:  av_1931_062_TEI.xml\n",
      "File:  av_1932_063_TEI.json\n",
      "File:  av_1932_063_TEI.xml\n",
      "File:  av_1933_064_TEI.json\n",
      "File:  av_1933_064_TEI.xml\n",
      "File:  av_1934_065_TEI.json\n",
      "File:  av_1934_065_TEI.xml\n",
      "File:  av_1935_066_TEI.json\n",
      "File:  av_1935_066_TEI.xml\n",
      "File:  av_1936_067_TEI.json\n",
      "File:  av_1936_067_TEI.xml\n",
      "File:  av_1937_068_TEI.json\n",
      "File:  av_1937_068_TEI.xml\n",
      "File:  av_1938_069_TEI.json\n",
      "File:  av_1938_069_TEI.xml\n",
      "File:  av_1939_070_TEI.json\n",
      "File:  av_1939_070_TEI.xml\n",
      "File:  av_1940_071_TEI.json\n",
      "File:  av_1940_071_TEI.xml\n",
      "File:  av_1941_072_TEI.json\n",
      "File:  av_1941_072_TEI.xml\n",
      "File:  av_1942_073_TEI.json\n",
      "File:  av_1942_073_TEI.xml\n",
      "File:  av_1943_000_TEI.json\n",
      "File:  av_1943_000_TEI.xml\n",
      "File:  av_1949_074_TEI.json\n",
      "File:  av_1949_074_TEI.xml\n",
      "File:  av_1950_075_TEI.json\n",
      "File:  av_1950_075_TEI.xml\n",
      "File extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:47:18,152 : INFO : built Dictionary(64967 unique tokens: ['11.', 'Commerzienrathes', 'Dementsprechend', 'Deutschen', 'Hohe']...) from 682 documents (total 696571 corpus positions)\n",
      "2020-11-16 23:47:18,152 : INFO : saving Dictionary object under results_alpenwort_adj\\model.dict, separately None\n",
      "2020-11-16 23:47:18,192 : INFO : saved results_alpenwort_adj\\model.dict\n",
      "2020-11-16 23:47:18,782 : INFO : storing corpus in Matrix Market format to results_alpenwort_adj\\model.corp\n",
      "2020-11-16 23:47:18,782 : INFO : saving sparse matrix to results_alpenwort_adj\\model.corp\n",
      "2020-11-16 23:47:18,782 : INFO : PROGRESS: saving document #0\n",
      "2020-11-16 23:47:19,392 : INFO : saved 682x64967 matrix, density=1.026% (454421/44307494)\n",
      "2020-11-16 23:47:19,392 : INFO : saving MmCorpus index to results_alpenwort_adj\\model.corp.index\n",
      "2020-11-16 23:47:19,402 : INFO : using autotuned alpha, starting with [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]\n",
      "2020-11-16 23:47:19,402 : INFO : using symmetric eta at 0.02\n",
      "2020-11-16 23:47:19,412 : INFO : using serial LDA version on this node\n",
      "2020-11-16 23:47:19,792 : INFO : running online (multi-pass) LDA training, 50 topics, 20 passes over the supplied corpus of 682 documents, updating model once every 682 documents, evaluating perplexity every 682 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-11-16 23:47:24,302 : INFO : -23.195 per-word bound, 9602658.2 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:47:24,302 : INFO : PROGRESS: pass 0, at document #682/682\n",
      "2020-11-16 23:47:26,802 : INFO : optimized alpha [0.034421533, 0.034914672, 0.036183517, 0.03313653, 0.034437582, 0.033896394, 0.033156395, 0.034168057, 0.034955725, 0.03398199, 0.03482639, 0.034128603, 0.033866536, 0.03470982, 0.030451417, 0.03625711, 0.036217578, 0.034543425, 0.036064707, 0.03457546, 0.03410185, 0.030526072, 0.032581396, 0.03351208, 0.03420241, 0.031921662, 0.035428654, 0.03501655, 0.033385552, 0.034136456, 0.03526888, 0.03282064, 0.03333583, 0.032538883, 0.036338385, 0.036658593, 0.030101642, 0.032779228, 0.03652785, 0.034937184, 0.028455826, 0.033156376, 0.033797003, 0.032228753, 0.0319546, 0.03617397, 0.032812916, 0.035408292, 0.033527743, 0.035388052]\n",
      "2020-11-16 23:47:27,162 : INFO : topic #40 (0.028): 0.004*\"großen\" + 0.004*\"ganzen\" + 0.004*\"erste\" + 0.004*\"ganze\" + 0.003*\"kleinen\" + 0.003*\"ersten\" + 0.003*\"gut\" + 0.003*\"alten\" + 0.003*\"nächsten\" + 0.003*\"später\"\n",
      "2020-11-16 23:47:27,172 : INFO : topic #36 (0.030): 0.005*\"ersten\" + 0.004*\"großen\" + 0.004*\"gut\" + 0.004*\"weit\" + 0.004*\"hoch\" + 0.004*\"leicht\" + 0.003*\"ganzen\" + 0.003*\"große\" + 0.003*\"letzten\" + 0.003*\"rasch\"\n",
      "2020-11-16 23:47:27,172 : INFO : topic #34 (0.036): 0.007*\"großen\" + 0.006*\"ersten\" + 0.006*\"große\" + 0.006*\"ganzen\" + 0.006*\"weit\" + 0.005*\"gut\" + 0.005*\"kleinen\" + 0.005*\"steilen\" + 0.004*\"hoch\" + 0.004*\"später\"\n",
      "2020-11-16 23:47:27,172 : INFO : topic #38 (0.037): 0.009*\"großen\" + 0.007*\"ersten\" + 0.005*\"große\" + 0.005*\"weit\" + 0.005*\"ganzen\" + 0.004*\"anderen\" + 0.004*\"leicht\" + 0.004*\"kleinen\" + 0.004*\"ganze\" + 0.004*\"hoch\"\n",
      "2020-11-16 23:47:27,172 : INFO : topic #35 (0.037): 0.008*\"großen\" + 0.008*\"ersten\" + 0.007*\"weit\" + 0.005*\"später\" + 0.005*\"große\" + 0.004*\"ganze\" + 0.004*\"letzten\" + 0.004*\"anderen\" + 0.004*\"kleinen\" + 0.004*\"leicht\"\n",
      "2020-11-16 23:47:27,192 : INFO : topic diff=20.522045, rho=1.000000\n",
      "2020-11-16 23:47:32,232 : INFO : -12.481 per-word bound, 5715.2 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:47:32,232 : INFO : PROGRESS: pass 1, at document #682/682\n",
      "2020-11-16 23:47:34,822 : INFO : optimized alpha [0.030095007, 0.03071864, 0.04111605, 0.02906175, 0.030476488, 0.02973136, 0.028984267, 0.030105004, 0.030658, 0.02986357, 0.031272568, 0.030476827, 0.029674735, 0.030545589, 0.02693135, 0.040941875, 0.045136612, 0.03061098, 0.040940262, 0.030439392, 0.029941035, 0.027018832, 0.02855248, 0.02935089, 0.0299302, 0.028114889, 0.034910016, 0.03185126, 0.02936349, 0.029938474, 0.03130829, 0.028824084, 0.029118508, 0.028520472, 0.04064656, 0.049215022, 0.026661849, 0.028829094, 0.0459966, 0.031895846, 0.025381448, 0.029114824, 0.029559504, 0.028319594, 0.028110225, 0.042602025, 0.028759602, 0.03385936, 0.029359154, 0.03296367]\n",
      "2020-11-16 23:47:35,182 : INFO : topic #40 (0.025): 0.003*\"großen\" + 0.003*\"ganzen\" + 0.003*\"erste\" + 0.003*\"ganze\" + 0.002*\"kleinen\" + 0.002*\"ersten\" + 0.002*\"gut\" + 0.002*\"alten\" + 0.002*\"nächsten\" + 0.002*\"später\"\n",
      "2020-11-16 23:47:35,182 : INFO : topic #36 (0.027): 0.004*\"ersten\" + 0.003*\"großen\" + 0.003*\"gut\" + 0.003*\"weit\" + 0.003*\"hoch\" + 0.003*\"leicht\" + 0.003*\"ganzen\" + 0.002*\"große\" + 0.002*\"letzten\" + 0.002*\"rasch\"\n",
      "2020-11-16 23:47:35,182 : INFO : topic #16 (0.045): 0.007*\"ersten\" + 0.006*\"leicht\" + 0.005*\"gut\" + 0.005*\"ganzen\" + 0.005*\"steilen\" + 0.005*\"weit\" + 0.005*\"hoch\" + 0.005*\"kleinen\" + 0.004*\"letzten\" + 0.004*\"großen\"\n",
      "2020-11-16 23:47:35,192 : INFO : topic #38 (0.046): 0.010*\"großen\" + 0.007*\"ersten\" + 0.006*\"große\" + 0.005*\"hoch\" + 0.005*\"weit\" + 0.005*\"ganzen\" + 0.005*\"anderen\" + 0.005*\"kleinen\" + 0.005*\"ganze\" + 0.004*\"hohen\"\n",
      "2020-11-16 23:47:35,192 : INFO : topic #35 (0.049): 0.008*\"großen\" + 0.008*\"ersten\" + 0.007*\"weit\" + 0.005*\"letzten\" + 0.005*\"große\" + 0.005*\"kleinen\" + 0.005*\"später\" + 0.005*\"ganze\" + 0.005*\"nächsten\" + 0.005*\"anderen\"\n",
      "2020-11-16 23:47:35,202 : INFO : topic diff=6.863572, rho=0.577350\n",
      "2020-11-16 23:47:40,322 : INFO : -11.568 per-word bound, 3036.7 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:47:40,322 : INFO : PROGRESS: pass 2, at document #682/682\n",
      "2020-11-16 23:47:43,592 : INFO : optimized alpha [0.027144652, 0.027644577, 0.04454053, 0.0262634, 0.027610723, 0.02682378, 0.026176045, 0.027254764, 0.027645733, 0.02700637, 0.028237091, 0.028595366, 0.026807625, 0.027530538, 0.024506418, 0.042406365, 0.052749872, 0.027637627, 0.04082175, 0.027474418, 0.027070053, 0.02460057, 0.025827212, 0.026545476, 0.026984561, 0.025494818, 0.032076504, 0.02873995, 0.026532408, 0.02701716, 0.028374616, 0.026070744, 0.026284259, 0.025801301, 0.04013665, 0.062416136, 0.024285162, 0.02612523, 0.052782405, 0.029020099, 0.023227336, 0.026357487, 0.026664725, 0.025685903, 0.025468621, 0.044424046, 0.026018973, 0.030698467, 0.026502488, 0.029778222]\n",
      "2020-11-16 23:47:44,012 : INFO : topic #40 (0.023): 0.002*\"großen\" + 0.002*\"ganzen\" + 0.002*\"erste\" + 0.002*\"ganze\" + 0.002*\"kleinen\" + 0.002*\"ersten\" + 0.002*\"gut\" + 0.002*\"alten\" + 0.002*\"nächsten\" + 0.001*\"später\"\n",
      "2020-11-16 23:47:44,012 : INFO : topic #36 (0.024): 0.003*\"ersten\" + 0.002*\"großen\" + 0.002*\"gut\" + 0.002*\"weit\" + 0.002*\"hoch\" + 0.002*\"leicht\" + 0.002*\"ganzen\" + 0.002*\"große\" + 0.002*\"letzten\" + 0.002*\"rasch\"\n",
      "2020-11-16 23:47:44,022 : INFO : topic #16 (0.053): 0.007*\"ersten\" + 0.006*\"leicht\" + 0.005*\"steilen\" + 0.005*\"ganzen\" + 0.005*\"gut\" + 0.005*\"kleinen\" + 0.005*\"weit\" + 0.005*\"erste\" + 0.005*\"hoch\" + 0.004*\"steile\"\n",
      "2020-11-16 23:47:44,022 : INFO : topic #38 (0.053): 0.010*\"großen\" + 0.007*\"große\" + 0.006*\"ersten\" + 0.006*\"hoch\" + 0.005*\"weit\" + 0.005*\"anderen\" + 0.005*\"ganzen\" + 0.005*\"kleinen\" + 0.005*\"ganze\" + 0.004*\"hohen\"\n",
      "2020-11-16 23:47:44,022 : INFO : topic #35 (0.062): 0.008*\"großen\" + 0.008*\"ersten\" + 0.007*\"weit\" + 0.006*\"letzten\" + 0.005*\"große\" + 0.005*\"kleinen\" + 0.005*\"nächsten\" + 0.005*\"ganze\" + 0.005*\"später\" + 0.005*\"gut\"\n",
      "2020-11-16 23:47:44,042 : INFO : topic diff=5.033697, rho=0.500000\n",
      "2020-11-16 23:47:51,848 : INFO : -11.390 per-word bound, 2684.4 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:47:51,858 : INFO : PROGRESS: pass 3, at document #682/682\n",
      "2020-11-16 23:47:56,274 : INFO : optimized alpha [0.024985898, 0.025405217, 0.04882667, 0.024204852, 0.025522474, 0.024692815, 0.024111614, 0.025197066, 0.02542773, 0.024925414, 0.026001865, 0.02779217, 0.024703259, 0.025329182, 0.02269671, 0.043266572, 0.059531543, 0.025480645, 0.040673777, 0.025305191, 0.02498071, 0.022794882, 0.023817386, 0.024481533, 0.024829015, 0.023554206, 0.029893583, 0.026473062, 0.024452103, 0.024858207, 0.02624328, 0.02404194, 0.024202745, 0.023795497, 0.03969795, 0.07535962, 0.02250795, 0.02414649, 0.058269214, 0.026901769, 0.021601455, 0.024325069, 0.024543108, 0.02373606, 0.02351416, 0.045654126, 0.023998609, 0.028351426, 0.024405852, 0.027441138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:47:56,704 : INFO : topic #40 (0.022): 0.001*\"großen\" + 0.001*\"ganzen\" + 0.001*\"erste\" + 0.001*\"ganze\" + 0.001*\"kleinen\" + 0.001*\"ersten\" + 0.001*\"gut\" + 0.001*\"alten\" + 0.001*\"nächsten\" + 0.001*\"später\"\n",
      "2020-11-16 23:47:56,704 : INFO : topic #36 (0.023): 0.002*\"ersten\" + 0.002*\"großen\" + 0.002*\"gut\" + 0.001*\"weit\" + 0.001*\"hoch\" + 0.001*\"leicht\" + 0.001*\"ganzen\" + 0.001*\"große\" + 0.001*\"letzten\" + 0.001*\"rasch\"\n",
      "2020-11-16 23:47:56,704 : INFO : topic #38 (0.058): 0.011*\"großen\" + 0.007*\"große\" + 0.006*\"hoch\" + 0.006*\"ersten\" + 0.005*\"weit\" + 0.005*\"anderen\" + 0.005*\"ganzen\" + 0.005*\"ganze\" + 0.005*\"kleinen\" + 0.005*\"hohen\"\n",
      "2020-11-16 23:47:56,714 : INFO : topic #16 (0.060): 0.007*\"ersten\" + 0.007*\"leicht\" + 0.005*\"steilen\" + 0.005*\"kleinen\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"gut\" + 0.005*\"erste\" + 0.005*\"steile\" + 0.005*\"hoch\"\n",
      "2020-11-16 23:47:56,714 : INFO : topic #35 (0.075): 0.008*\"großen\" + 0.008*\"ersten\" + 0.007*\"weit\" + 0.006*\"letzten\" + 0.005*\"kleinen\" + 0.005*\"große\" + 0.005*\"nächsten\" + 0.005*\"gut\" + 0.005*\"ganze\" + 0.005*\"später\"\n",
      "2020-11-16 23:47:56,724 : INFO : topic diff=3.618745, rho=0.447214\n",
      "2020-11-16 23:48:03,834 : INFO : -11.426 per-word bound, 2751.7 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:48:03,844 : INFO : PROGRESS: pass 4, at document #682/682\n",
      "2020-11-16 23:48:08,128 : INFO : optimized alpha [0.02332769, 0.023690227, 0.053379204, 0.022603363, 0.023961643, 0.023039723, 0.022506252, 0.023595283, 0.02372493, 0.023322538, 0.024274025, 0.027556987, 0.023068778, 0.02362587, 0.021274086, 0.04407661, 0.065927066, 0.023808522, 0.04084126, 0.023624223, 0.023386968, 0.021374948, 0.022250941, 0.02287581, 0.023158586, 0.022037165, 0.028252874, 0.0247706, 0.022849936, 0.023185344, 0.0246695, 0.022461725, 0.02258524, 0.022231916, 0.039298035, 0.08855051, 0.021108868, 0.022601493, 0.062908195, 0.02530427, 0.02031272, 0.022741664, 0.022896877, 0.022212146, 0.021987304, 0.046547998, 0.022424405, 0.02659477, 0.022777349, 0.025639927]\n",
      "2020-11-16 23:48:08,640 : INFO : topic #40 (0.020): 0.001*\"großen\" + 0.001*\"ganzen\" + 0.001*\"erste\" + 0.001*\"ganze\" + 0.001*\"kleinen\" + 0.001*\"ersten\" + 0.001*\"gut\" + 0.001*\"alten\" + 0.001*\"nächsten\" + 0.001*\"später\"\n",
      "2020-11-16 23:48:08,640 : INFO : topic #36 (0.021): 0.001*\"ersten\" + 0.001*\"großen\" + 0.001*\"gut\" + 0.001*\"weit\" + 0.001*\"hoch\" + 0.001*\"leicht\" + 0.001*\"ganzen\" + 0.001*\"große\" + 0.001*\"letzten\" + 0.001*\"rasch\"\n",
      "2020-11-16 23:48:08,640 : INFO : topic #38 (0.063): 0.011*\"großen\" + 0.007*\"große\" + 0.006*\"hoch\" + 0.006*\"ersten\" + 0.005*\"weit\" + 0.005*\"anderen\" + 0.005*\"ganze\" + 0.005*\"ganzen\" + 0.005*\"hohen\" + 0.005*\"kleinen\"\n",
      "2020-11-16 23:48:08,650 : INFO : topic #16 (0.066): 0.007*\"ersten\" + 0.007*\"leicht\" + 0.005*\"steilen\" + 0.005*\"kleinen\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"erste\" + 0.005*\"steile\" + 0.005*\"gut\" + 0.004*\"hoch\"\n",
      "2020-11-16 23:48:08,650 : INFO : topic #35 (0.089): 0.008*\"großen\" + 0.008*\"ersten\" + 0.007*\"weit\" + 0.006*\"letzten\" + 0.005*\"kleinen\" + 0.005*\"gut\" + 0.005*\"große\" + 0.005*\"nächsten\" + 0.005*\"ganze\" + 0.005*\"später\"\n",
      "2020-11-16 23:48:08,670 : INFO : topic diff=2.585011, rho=0.408248\n",
      "2020-11-16 23:48:16,294 : INFO : -11.511 per-word bound, 2918.0 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:48:16,304 : INFO : PROGRESS: pass 5, at document #682/682\n",
      "2020-11-16 23:48:21,474 : INFO : optimized alpha [0.021990428, 0.022310391, 0.058177236, 0.021308688, 0.022724459, 0.021706399, 0.021208862, 0.022325817, 0.022357134, 0.022026524, 0.022897715, 0.027614387, 0.021749096, 0.02225463, 0.020114765, 0.04479499, 0.07195179, 0.022487616, 0.041256856, 0.022282867, 0.02211065, 0.020217553, 0.020982785, 0.021577733, 0.021812242, 0.02080623, 0.026942706, 0.023413561, 0.021554785, 0.021837112, 0.023423621, 0.021183094, 0.02127873, 0.02096593, 0.03910511, 0.10171576, 0.019967467, 0.021349058, 0.06690559, 0.024094082, 0.019255742, 0.021460246, 0.021568684, 0.020988114, 0.020748993, 0.04744487, 0.021150287, 0.025228152, 0.02146244, 0.024267275]\n",
      "2020-11-16 23:48:21,894 : INFO : topic #40 (0.019): 0.001*\"großen\" + 0.001*\"ganzen\" + 0.001*\"erste\" + 0.001*\"ganze\" + 0.001*\"kleinen\" + 0.001*\"ersten\" + 0.001*\"gut\" + 0.001*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:48:21,894 : INFO : topic #36 (0.020): 0.001*\"ersten\" + 0.001*\"großen\" + 0.001*\"gut\" + 0.001*\"weit\" + 0.001*\"hoch\" + 0.001*\"leicht\" + 0.001*\"ganzen\" + 0.001*\"große\" + 0.001*\"letzten\" + 0.001*\"rasch\"\n",
      "2020-11-16 23:48:21,904 : INFO : topic #38 (0.067): 0.011*\"großen\" + 0.007*\"große\" + 0.007*\"hoch\" + 0.006*\"ersten\" + 0.006*\"weit\" + 0.005*\"anderen\" + 0.005*\"hohen\" + 0.005*\"ganze\" + 0.005*\"ganzen\" + 0.005*\"alten\"\n",
      "2020-11-16 23:48:21,904 : INFO : topic #16 (0.072): 0.007*\"ersten\" + 0.007*\"leicht\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"ganzen\" + 0.005*\"erste\" + 0.005*\"weit\" + 0.005*\"steile\" + 0.005*\"gut\" + 0.005*\"steil\"\n",
      "2020-11-16 23:48:21,904 : INFO : topic #35 (0.102): 0.008*\"großen\" + 0.008*\"ersten\" + 0.007*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"kleinen\" + 0.005*\"nächsten\" + 0.005*\"große\" + 0.005*\"ganze\" + 0.005*\"hoch\"\n",
      "2020-11-16 23:48:21,914 : INFO : topic diff=1.858351, rho=0.377964\n",
      "2020-11-16 23:48:29,664 : INFO : -11.591 per-word bound, 3085.2 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:48:29,664 : INFO : PROGRESS: pass 6, at document #682/682\n",
      "2020-11-16 23:48:34,764 : INFO : optimized alpha [0.020892348, 0.02116769, 0.06325603, 0.020232424, 0.021705128, 0.02060004, 0.020130575, 0.021269076, 0.021223744, 0.020949027, 0.021769356, 0.027875584, 0.020664375, 0.02111854, 0.01914479, 0.045509204, 0.077926174, 0.021390699, 0.041809708, 0.021181589, 0.021048779, 0.019249037, 0.019927345, 0.020498669, 0.020695714, 0.01977991, 0.025937285, 0.022288281, 0.020478075, 0.020719074, 0.02241015, 0.020142147, 0.020193323, 0.01991219, 0.038926624, 0.11478433, 0.01901164, 0.020305634, 0.070404895, 0.023113156, 0.018366793, 0.020394107, 0.02046629, 0.019978339, 0.01971693, 0.048257686, 0.020090101, 0.024109451, 0.020370407, 0.023169884]\n",
      "2020-11-16 23:48:35,184 : INFO : topic #40 (0.018): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:48:35,184 : INFO : topic #36 (0.019): 0.001*\"ersten\" + 0.001*\"großen\" + 0.001*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:48:35,184 : INFO : topic #38 (0.070): 0.011*\"großen\" + 0.007*\"große\" + 0.007*\"hoch\" + 0.006*\"weit\" + 0.005*\"ersten\" + 0.005*\"anderen\" + 0.005*\"hohen\" + 0.005*\"ganze\" + 0.005*\"alten\" + 0.005*\"ganzen\"\n",
      "2020-11-16 23:48:35,194 : INFO : topic #16 (0.078): 0.007*\"ersten\" + 0.007*\"leicht\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"ganzen\" + 0.005*\"erste\" + 0.005*\"steile\" + 0.005*\"weit\" + 0.005*\"steil\" + 0.005*\"gut\"\n",
      "2020-11-16 23:48:35,194 : INFO : topic #35 (0.115): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"kleinen\" + 0.006*\"nächsten\" + 0.005*\"große\" + 0.005*\"ganze\" + 0.005*\"hoch\"\n",
      "2020-11-16 23:48:35,204 : INFO : topic diff=1.349777, rho=0.353553\n",
      "2020-11-16 23:48:42,674 : INFO : -11.654 per-word bound, 3222.5 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:48:42,674 : INFO : PROGRESS: pass 7, at document #682/682\n",
      "2020-11-16 23:48:47,704 : INFO : optimized alpha [0.019960787, 0.020200301, 0.068473265, 0.019318396, 0.020847538, 0.019661894, 0.019214986, 0.020370597, 0.020263728, 0.02004425, 0.020824116, 0.028203437, 0.019743599, 0.020156402, 0.018316623, 0.046215903, 0.08384242, 0.020470483, 0.04252369, 0.020247392, 0.020146258, 0.018422022, 0.019030135, 0.019582303, 0.019749379, 0.018906154, 0.025123972, 0.021345565, 0.019563638, 0.019771477, 0.02154714, 0.019256733, 0.019272024, 0.019016344, 0.0389002, 0.12768292, 0.018194947, 0.01941792, 0.07347596, 0.022276863, 0.017604535, 0.01948806, 0.01953126, 0.01911794, 0.018838558, 0.048994888, 0.01918902, 0.023173546, 0.01944372, 0.022239469]\n",
      "2020-11-16 23:48:48,134 : INFO : topic #40 (0.018): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:48:48,134 : INFO : topic #36 (0.018): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:48:48,134 : INFO : topic #38 (0.073): 0.012*\"großen\" + 0.007*\"große\" + 0.007*\"hoch\" + 0.006*\"weit\" + 0.005*\"anderen\" + 0.005*\"ersten\" + 0.005*\"hohen\" + 0.005*\"alten\" + 0.005*\"ganze\" + 0.005*\"ganzen\"\n",
      "2020-11-16 23:48:48,134 : INFO : topic #16 (0.084): 0.007*\"ersten\" + 0.007*\"leicht\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"steile\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"steil\" + 0.005*\"gut\"\n",
      "2020-11-16 23:48:48,134 : INFO : topic #35 (0.128): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.005*\"große\" + 0.005*\"hoch\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:48:48,154 : INFO : topic diff=0.993816, rho=0.333333\n",
      "2020-11-16 23:48:55,646 : INFO : -11.698 per-word bound, 3322.7 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:48:55,656 : INFO : PROGRESS: pass 8, at document #682/682\n",
      "2020-11-16 23:49:00,636 : INFO : optimized alpha [0.019156972, 0.019367067, 0.07371893, 0.018529017, 0.020124149, 0.018852731, 0.018424358, 0.01961283, 0.01945534, 0.01926255, 0.020009255, 0.028539587, 0.018948639, 0.019336931, 0.01759816, 0.04689439, 0.089821346, 0.01969517, 0.043287985, 0.019441364, 0.019375598, 0.01770449, 0.018254641, 0.018790934, 0.018933466, 0.018149983, 0.0244324, 0.020548008, 0.01877389, 0.0189545, 0.020800157, 0.018491006, 0.018476702, 0.01824197, 0.038984187, 0.14029042, 0.017485993, 0.018650142, 0.07628, 0.021552255, 0.016940821, 0.018705145, 0.018724611, 0.018390706, 0.018078588, 0.049734008, 0.018410295, 0.02240988, 0.018643947, 0.021447528]\n",
      "2020-11-16 23:49:01,066 : INFO : topic #40 (0.017): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:49:01,066 : INFO : topic #36 (0.017): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:49:01,066 : INFO : topic #38 (0.076): 0.012*\"großen\" + 0.008*\"große\" + 0.007*\"hoch\" + 0.006*\"weit\" + 0.005*\"anderen\" + 0.005*\"ersten\" + 0.005*\"alten\" + 0.005*\"hohen\" + 0.005*\"ganze\" + 0.005*\"ganzen\"\n",
      "2020-11-16 23:49:01,066 : INFO : topic #16 (0.090): 0.007*\"ersten\" + 0.007*\"leicht\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"steile\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"steil\" + 0.005*\"gut\"\n",
      "2020-11-16 23:49:01,066 : INFO : topic #35 (0.140): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.005*\"große\" + 0.005*\"hoch\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:49:01,086 : INFO : topic diff=0.740229, rho=0.316228\n",
      "2020-11-16 23:49:08,456 : INFO : -11.728 per-word bound, 3392.6 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:49:08,466 : INFO : PROGRESS: pass 9, at document #682/682\n",
      "2020-11-16 23:49:13,438 : INFO : optimized alpha [0.018453699, 0.018647749, 0.07911611, 0.017837843, 0.01948888, 0.018145023, 0.01773216, 0.018948473, 0.0187486, 0.018586256, 0.019296816, 0.028975876, 0.018252753, 0.018620588, 0.016966622, 0.047561243, 0.09563435, 0.01902481, 0.04420973, 0.018736206, 0.018700397, 0.017073726, 0.017575134, 0.018098054, 0.018220086, 0.0174867, 0.023848506, 0.01987233, 0.01808238, 0.018240212, 0.020144954, 0.017819723, 0.017780574, 0.017563403, 0.039083846, 0.15238313, 0.016862474, 0.017977042, 0.07880246, 0.020945137, 0.016355561, 0.018019315, 0.018018961, 0.01775191, 0.017412117, 0.050439473, 0.017728064, 0.021741973, 0.01794407, 0.020764347]\n",
      "2020-11-16 23:49:13,868 : INFO : topic #40 (0.016): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:49:13,868 : INFO : topic #36 (0.017): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:49:13,868 : INFO : topic #2 (0.079): 0.009*\"großen\" + 0.007*\"alten\" + 0.007*\"alpinen\" + 0.007*\"große\" + 0.006*\"früher\" + 0.006*\"anderen\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:49:13,868 : INFO : topic #16 (0.096): 0.007*\"ersten\" + 0.007*\"leicht\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"steile\" + 0.005*\"erste\" + 0.005*\"steil\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"gut\"\n",
      "2020-11-16 23:49:13,878 : INFO : topic #35 (0.152): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.005*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:49:13,888 : INFO : topic diff=0.557820, rho=0.301511\n",
      "2020-11-16 23:49:20,888 : INFO : -11.747 per-word bound, 3437.7 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:49:20,888 : INFO : PROGRESS: pass 10, at document #682/682\n",
      "2020-11-16 23:49:25,758 : INFO : optimized alpha [0.01783122, 0.0180117, 0.08455176, 0.017225662, 0.018932965, 0.017518803, 0.017119125, 0.018359283, 0.018123254, 0.017986879, 0.018674605, 0.029493246, 0.01764412, 0.017987002, 0.016405353, 0.048194844, 0.10103378, 0.018430598, 0.04528066, 0.018112108, 0.0181019, 0.016513111, 0.016972903, 0.017484391, 0.017596604, 0.016898302, 0.023357533, 0.019282393, 0.017469894, 0.017608365, 0.019572364, 0.017231926, 0.017164191, 0.016961977, 0.039201997, 0.16406739, 0.016308075, 0.017395282, 0.08096498, 0.020405957, 0.015833976, 0.017411627, 0.017394451, 0.017191889, 0.016821003, 0.051083367, 0.017123496, 0.021150503, 0.01732449, 0.02018611]\n",
      "2020-11-16 23:49:26,158 : INFO : topic #40 (0.016): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:49:26,168 : INFO : topic #36 (0.016): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:49:26,168 : INFO : topic #2 (0.085): 0.009*\"großen\" + 0.007*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"früher\" + 0.006*\"anderen\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:49:26,168 : INFO : topic #16 (0.101): 0.008*\"leicht\" + 0.007*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"steile\" + 0.005*\"erste\" + 0.005*\"steil\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:49:26,168 : INFO : topic #35 (0.164): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.005*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:49:26,178 : INFO : topic diff=0.424563, rho=0.288675\n",
      "2020-11-16 23:49:33,368 : INFO : -11.759 per-word bound, 3467.1 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:49:33,368 : INFO : PROGRESS: pass 11, at document #682/682\n",
      "2020-11-16 23:49:38,308 : INFO : optimized alpha [0.017275056, 0.017451078, 0.09018651, 0.016678356, 0.018442584, 0.016959429, 0.01657109, 0.017839441, 0.017564654, 0.017450728, 0.018126486, 0.030051518, 0.017099936, 0.01742126, 0.015902044, 0.04883219, 0.106237605, 0.01789896, 0.046434917, 0.017554535, 0.017566493, 0.016010368, 0.016434187, 0.016935792, 0.017039653, 0.01637153, 0.022940623, 0.018754758, 0.016922314, 0.017044116, 0.019067656, 0.016705759, 0.016613271, 0.016423954, 0.039321646, 0.17525582, 0.015810717, 0.016874423, 0.08298438, 0.019922705, 0.015365107, 0.016868157, 0.016836513, 0.016689835, 0.016291883, 0.051672958, 0.016582761, 0.020621892, 0.016770808, 0.01968552]\n",
      "2020-11-16 23:49:38,708 : INFO : topic #40 (0.015): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:49:38,708 : INFO : topic #36 (0.016): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:49:38,708 : INFO : topic #2 (0.090): 0.009*\"großen\" + 0.007*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:49:38,718 : INFO : topic #16 (0.106): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"steile\" + 0.005*\"steil\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:49:38,718 : INFO : topic #35 (0.175): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.005*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:49:38,728 : INFO : topic diff=0.326763, rho=0.277350\n",
      "2020-11-16 23:49:45,908 : INFO : -11.767 per-word bound, 3484.6 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:49:45,908 : INFO : PROGRESS: pass 12, at document #682/682\n",
      "2020-11-16 23:49:51,048 : INFO : optimized alpha [0.016773973, 0.016946211, 0.09575547, 0.016184978, 0.017999599, 0.016455557, 0.016077086, 0.017370284, 0.017061478, 0.016967176, 0.017632658, 0.03071491, 0.016616082, 0.016911829, 0.015447107, 0.049438477, 0.11124652, 0.01742639, 0.047605187, 0.017052222, 0.017083587, 0.015555923, 0.015948305, 0.016441274, 0.016537916, 0.015896078, 0.02256461, 0.01828624, 0.016428685, 0.016535975, 0.018627295, 0.016230883, 0.016116748, 0.01593868, 0.03943429, 0.18602327, 0.015360987, 0.01640419, 0.084777415, 0.019486139, 0.0149403615, 0.016378097, 0.016333861, 0.016236091, 0.015814379, 0.052254662, 0.01609511, 0.020145569, 0.016271876, 0.019233987]\n",
      "2020-11-16 23:49:51,448 : INFO : topic #40 (0.015): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:49:51,448 : INFO : topic #36 (0.015): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:49:51,448 : INFO : topic #2 (0.096): 0.009*\"großen\" + 0.007*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:49:51,448 : INFO : topic #16 (0.111): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"steile\" + 0.005*\"steil\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:49:51,448 : INFO : topic #35 (0.186): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.005*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:49:51,468 : INFO : topic diff=0.253463, rho=0.267261\n",
      "2020-11-16 23:49:58,728 : INFO : -11.770 per-word bound, 3492.0 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:49:58,728 : INFO : PROGRESS: pass 13, at document #682/682\n",
      "2020-11-16 23:50:03,622 : INFO : optimized alpha [0.016319223, 0.016488163, 0.101229936, 0.01573699, 0.017610228, 0.015998363, 0.015628554, 0.016943816, 0.016604897, 0.016527925, 0.017184438, 0.031394765, 0.016176533, 0.016449727, 0.0150330225, 0.050005365, 0.115967564, 0.016996965, 0.04881888, 0.016596384, 0.016644903, 0.015142274, 0.015506926, 0.01599228, 0.016082605, 0.015463886, 0.022222849, 0.017881397, 0.015980473, 0.016074998, 0.018226823, 0.015799234, 0.015665997, 0.015497833, 0.03951858, 0.19624053, 0.014951507, 0.015976638, 0.08642973, 0.019088991, 0.014552992, 0.015933009, 0.015877709, 0.015823133, 0.015380379, 0.05278507, 0.015652167, 0.019727595, 0.015819006, 0.018830996]\n",
      "2020-11-16 23:50:04,022 : INFO : topic #40 (0.015): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:50:04,022 : INFO : topic #36 (0.015): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:50:04,022 : INFO : topic #2 (0.101): 0.009*\"großen\" + 0.008*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:50:04,022 : INFO : topic #16 (0.116): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"steile\" + 0.005*\"steil\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:50:04,022 : INFO : topic #35 (0.196): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.005*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:50:04,042 : INFO : topic diff=0.198456, rho=0.258199\n",
      "2020-11-16 23:50:11,242 : INFO : -11.771 per-word bound, 3495.0 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:50:11,242 : INFO : PROGRESS: pass 14, at document #682/682\n",
      "2020-11-16 23:50:15,722 : INFO : optimized alpha [0.015903912, 0.016069943, 0.10661118, 0.015327664, 0.017253654, 0.015580882, 0.015218746, 0.016560039, 0.016187971, 0.016126432, 0.016775033, 0.032068275, 0.01577474, 0.016027879, 0.014653839, 0.050498176, 0.120339304, 0.016604275, 0.05003168, 0.016180111, 0.016243914, 0.014763479, 0.015103469, 0.015582055, 0.015666809, 0.015068595, 0.021918446, 0.01751112, 0.015570939, 0.015654152, 0.017867165, 0.015404453, 0.015254224, 0.015094847, 0.039600458, 0.20595877, 0.014576423, 0.015585501, 0.08794117, 0.018725537, 0.01419763, 0.015526239, 0.015461129, 0.015445017, 0.01498348, 0.053276584, 0.015247319, 0.01936074, 0.0154053485, 0.01846246]\n",
      "2020-11-16 23:50:16,122 : INFO : topic #40 (0.014): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:50:16,122 : INFO : topic #36 (0.015): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:50:16,132 : INFO : topic #2 (0.107): 0.009*\"großen\" + 0.008*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:50:16,132 : INFO : topic #16 (0.120): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.005*\"steile\" + 0.005*\"steil\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:50:16,132 : INFO : topic #35 (0.206): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.005*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:50:16,142 : INFO : topic diff=0.157170, rho=0.250000\n",
      "2020-11-16 23:50:22,642 : INFO : -11.770 per-word bound, 3493.6 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:50:22,642 : INFO : PROGRESS: pass 15, at document #682/682\n",
      "2020-11-16 23:50:26,762 : INFO : optimized alpha [0.015522566, 0.01568601, 0.112027824, 0.014951646, 0.01692534, 0.015197593, 0.0148423, 0.016207082, 0.015805181, 0.015757488, 0.016404929, 0.032756187, 0.015405492, 0.015640676, 0.014304811, 0.05095511, 0.12449748, 0.01624328, 0.051220153, 0.015797902, 0.015875423, 0.014414798, 0.014732702, 0.015205232, 0.015285028, 0.014705137, 0.021654101, 0.017170506, 0.015194728, 0.01526784, 0.017536188, 0.015041475, 0.01487602, 0.014724507, 0.03967039, 0.2150425, 0.014231073, 0.015225785, 0.08934084, 0.018397752, 0.013869989, 0.015152491, 0.015078625, 0.015097001, 0.014618577, 0.053708863, 0.014875304, 0.019024374, 0.015025465, 0.018130366]\n",
      "2020-11-16 23:50:27,162 : INFO : topic #40 (0.014): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:50:27,162 : INFO : topic #36 (0.014): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:50:27,162 : INFO : topic #2 (0.112): 0.009*\"großen\" + 0.008*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:50:27,162 : INFO : topic #16 (0.124): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.006*\"steile\" + 0.006*\"steil\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:50:27,172 : INFO : topic #35 (0.215): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.006*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:50:27,182 : INFO : topic diff=0.125191, rho=0.242536\n",
      "2020-11-16 23:50:33,094 : INFO : -11.769 per-word bound, 3489.7 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:50:33,094 : INFO : PROGRESS: pass 16, at document #682/682\n",
      "2020-11-16 23:50:36,804 : INFO : optimized alpha [0.015170747, 0.015331877, 0.11737324, 0.014604604, 0.016621698, 0.014844023, 0.014494868, 0.015880987, 0.015452073, 0.015416882, 0.016069006, 0.03346356, 0.015064575, 0.0152835855, 0.013982083, 0.05137955, 0.1284659, 0.015909895, 0.052382603, 0.01544531, 0.015535231, 0.014092382, 0.014390384, 0.014857463, 0.0149328215, 0.0143694, 0.021418145, 0.016855745, 0.01485276, 0.014911541, 0.017230257, 0.014706194, 0.014527009, 0.014382567, 0.039747138, 0.22368784, 0.013911659, 0.014893449, 0.09068735, 0.01810115, 0.013566572, 0.014807479, 0.014725737, 0.014775242, 0.014281538, 0.054123092, 0.014531863, 0.018714016, 0.01467494, 0.01782995]\n",
      "2020-11-16 23:50:37,204 : INFO : topic #40 (0.014): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:50:37,204 : INFO : topic #36 (0.014): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:50:37,204 : INFO : topic #2 (0.117): 0.009*\"großen\" + 0.008*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:50:37,204 : INFO : topic #16 (0.128): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.006*\"steil\" + 0.006*\"steile\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:50:37,204 : INFO : topic #35 (0.224): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.006*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:50:37,224 : INFO : topic diff=0.100664, rho=0.235702\n",
      "2020-11-16 23:50:43,434 : INFO : -11.767 per-word bound, 3484.4 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:50:43,434 : INFO : PROGRESS: pass 17, at document #682/682\n",
      "2020-11-16 23:50:46,814 : INFO : optimized alpha [0.014844791, 0.015003835, 0.12255021, 0.014282944, 0.016339751, 0.014516473, 0.014172857, 0.0155784665, 0.015124956, 0.015101117, 0.01576293, 0.03417536, 0.014748494, 0.01495285, 0.0136824455, 0.0517919, 0.13223894, 0.0156007325, 0.05357217, 0.015118657, 0.01521985, 0.013793033, 0.014072998, 0.014540035, 0.014606512, 0.01405798, 0.02119969, 0.016563686, 0.014535849, 0.014581512, 0.01695191, 0.014395209, 0.014203569, 0.0140655255, 0.03980543, 0.23215356, 0.013615032, 0.014585132, 0.09194648, 0.017826049, 0.013284476, 0.01448765, 0.014398788, 0.014476543, 0.013968933, 0.054493513, 0.014213461, 0.018426396, 0.014350133, 0.017551307]\n",
      "2020-11-16 23:50:47,194 : INFO : topic #40 (0.013): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:50:47,204 : INFO : topic #36 (0.014): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:50:47,204 : INFO : topic #2 (0.123): 0.009*\"großen\" + 0.008*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:50:47,204 : INFO : topic #16 (0.132): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.006*\"steil\" + 0.006*\"steile\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:50:47,204 : INFO : topic #35 (0.232): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.006*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:50:47,224 : INFO : topic diff=0.082088, rho=0.229416\n",
      "2020-11-16 23:50:52,904 : INFO : -11.764 per-word bound, 3478.6 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:50:52,904 : INFO : PROGRESS: pass 18, at document #682/682\n",
      "2020-11-16 23:50:56,284 : INFO : optimized alpha [0.014541609, 0.014698767, 0.12753695, 0.013983649, 0.016076973, 0.014211838, 0.013873241, 0.015296742, 0.014820723, 0.014807252, 0.015478047, 0.0348969, 0.014454304, 0.014645315, 0.013403201, 0.05217638, 0.13584362, 0.015312932, 0.05474184, 0.0148148425, 0.014926339, 0.013514049, 0.013777589, 0.014244667, 0.014303004, 0.0137680005, 0.020996595, 0.016291667, 0.014240945, 0.014274608, 0.016692804, 0.01410565, 0.013902654, 0.013770431, 0.039850857, 0.2404647, 0.013338531, 0.014298005, 0.09309925, 0.017569827, 0.0130212335, 0.014190015, 0.014094679, 0.0141982, 0.013677877, 0.054828264, 0.0139171295, 0.018158812, 0.0140479775, 0.017291866]\n",
      "2020-11-16 23:50:56,674 : INFO : topic #40 (0.013): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:50:56,674 : INFO : topic #36 (0.013): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n",
      "2020-11-16 23:50:56,674 : INFO : topic #2 (0.128): 0.009*\"großen\" + 0.008*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:50:56,684 : INFO : topic #16 (0.136): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.006*\"steil\" + 0.006*\"steile\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:50:56,684 : INFO : topic #35 (0.240): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.006*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:50:56,694 : INFO : topic diff=0.067483, rho=0.223607\n",
      "2020-11-16 23:51:02,514 : INFO : -11.762 per-word bound, 3472.1 perplexity estimate based on a held-out corpus of 682 documents with 696571 words\n",
      "2020-11-16 23:51:02,514 : INFO : PROGRESS: pass 19, at document #682/682\n",
      "2020-11-16 23:51:05,834 : INFO : optimized alpha [0.014258611, 0.014414052, 0.13236585, 0.013704181, 0.015831236, 0.013927501, 0.013593478, 0.015033482, 0.014541535, 0.0145328045, 0.01521193, 0.03560361, 0.01417953, 0.014358332, 0.013142071, 0.052501924, 0.13930154, 0.01504409, 0.055894982, 0.01453127, 0.014652225, 0.013253163, 0.013501673, 0.013968812, 0.014019706, 0.013497054, 0.020807134, 0.01603745, 0.013970197, 0.013988192, 0.016450599, 0.013835103, 0.013621705, 0.013494799, 0.039882146, 0.2484894, 0.013079912, 0.014029686, 0.0941844, 0.017335974, 0.01277477, 0.01391206, 0.01381081, 0.013937944, 0.013405939, 0.05516566, 0.013640367, 0.017909005, 0.013765896, 0.017054867]\n",
      "2020-11-16 23:51:06,224 : INFO : topic #40 (0.013): 0.000*\"großen\" + 0.000*\"ganzen\" + 0.000*\"erste\" + 0.000*\"ganze\" + 0.000*\"kleinen\" + 0.000*\"ersten\" + 0.000*\"gut\" + 0.000*\"alten\" + 0.000*\"nächsten\" + 0.000*\"später\"\n",
      "2020-11-16 23:51:06,224 : INFO : topic #36 (0.013): 0.000*\"ersten\" + 0.000*\"großen\" + 0.000*\"gut\" + 0.000*\"weit\" + 0.000*\"hoch\" + 0.000*\"leicht\" + 0.000*\"ganzen\" + 0.000*\"große\" + 0.000*\"letzten\" + 0.000*\"rasch\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:51:06,224 : INFO : topic #2 (0.132): 0.009*\"großen\" + 0.008*\"alpinen\" + 0.007*\"alten\" + 0.007*\"große\" + 0.006*\"anderen\" + 0.006*\"früher\" + 0.006*\"ersten\" + 0.006*\"andere\" + 0.005*\"neue\" + 0.005*\"ganze\"\n",
      "2020-11-16 23:51:06,234 : INFO : topic #16 (0.139): 0.008*\"leicht\" + 0.008*\"ersten\" + 0.006*\"kleinen\" + 0.006*\"steilen\" + 0.006*\"steil\" + 0.006*\"steile\" + 0.005*\"erste\" + 0.005*\"ganzen\" + 0.005*\"weit\" + 0.005*\"oberen\"\n",
      "2020-11-16 23:51:06,234 : INFO : topic #35 (0.248): 0.008*\"großen\" + 0.008*\"ersten\" + 0.006*\"weit\" + 0.006*\"letzten\" + 0.006*\"gut\" + 0.006*\"nächsten\" + 0.006*\"kleinen\" + 0.006*\"große\" + 0.005*\"hoch\" + 0.005*\"steilen\"\n",
      "2020-11-16 23:51:06,244 : INFO : topic diff=0.056062, rho=0.218218\n",
      "2020-11-16 23:51:06,254 : INFO : saving LdaState object under results_alpenwort_adj\\model-1.lda.state, separately None\n",
      "2020-11-16 23:51:06,354 : INFO : saved results_alpenwort_adj\\model-1.lda.state\n",
      "2020-11-16 23:51:06,394 : INFO : saving LdaModel object under results_alpenwort_adj\\model-1.lda, separately ['expElogbeta', 'sstats']\n",
      "2020-11-16 23:51:06,394 : INFO : storing np array 'expElogbeta' to results_alpenwort_adj\\model-1.lda.expElogbeta.npy\n",
      "2020-11-16 23:51:06,404 : INFO : not storing attribute state\n",
      "2020-11-16 23:51:06,404 : INFO : not storing attribute dispatcher\n",
      "2020-11-16 23:51:06,404 : INFO : not storing attribute id2word\n",
      "2020-11-16 23:51:06,404 : INFO : saved results_alpenwort_adj\\model-1.lda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA saved as results_alpenwort_adj\\model-1.lda\n"
     ]
    }
   ],
   "source": [
    "number_of_topics = 50\n",
    "\n",
    "try:\n",
    "    os.makedirs(result_path)\n",
    "except OSError as exception:\n",
    "    if exception.errno != errno.EEXIST:\n",
    "        raise\n",
    "\n",
    "# load corpus\n",
    "corpus = []   \n",
    "try:\n",
    "    # load prepared corpus\n",
    "    corpus = corpora.MmCorpus(os.path.join(result_path, model_name + \".corp\"))\n",
    "    dictionary = corpora.Dictionary.load(os.path.join(result_path, model_name + \".dict\"))\n",
    "except FileNotFoundError:\n",
    "    # convert json corpus\n",
    "    for json_file in sorted(os.listdir(corpus_path)):\n",
    "        print(\"File: \", json_file)\n",
    "        if json_file.endswith(\".json\"):\n",
    "            # get data\n",
    "            json_data = open(os.path.join(corpus_path, json_file))\n",
    "            data = json.load(json_data)\n",
    "            json_data.close()\n",
    "            for entry in data:\n",
    "                try:\n",
    "                    corpus.append(entry[\"text\"].split())\n",
    "                except AttributeError:\n",
    "                    corpus.append(entry[\"text\"])\n",
    "\n",
    "    print(\"File extraction complete.\")\n",
    "\n",
    "    dictionary = corpora.Dictionary(corpus)\n",
    "    dictionary.save(os.path.join(result_path, model_name + \".dict\"))\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "    corpora.MmCorpus.serialize(os.path.join(result_path, model_name + \".corp\"), corpus)    \n",
    "\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=number_of_topics, alpha='auto', eval_every=5, passes=20)\n",
    "\n",
    "start = 1\n",
    "while os.path.isfile(os.path.join(result_path, model_name + \"-\" +str(start)+ \".lda\")):\n",
    "    start += 1\n",
    "\n",
    "lda.save(os.path.join(result_path, model_name + \"-\" +str(start)+ \".lda\"))\n",
    "\n",
    "print(\"LDA saved as\", os.path.join(result_path, model_name + \"-\" +str(start)+ \".lda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute topic distribution for corpus<a id='compute'></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:51:51,984 : INFO : loading Dictionary object from results_alpenwort_adj\\model.dict\n",
      "2020-11-16 23:51:52,074 : INFO : loaded results_alpenwort_adj\\model.dict\n",
      "2020-11-16 23:51:52,074 : INFO : loading LdaModel object from results_alpenwort_adj\\model-1.lda\n",
      "2020-11-16 23:51:52,084 : INFO : loading expElogbeta from results_alpenwort_adj\\model-1.lda.expElogbeta.npy with mmap=None\n",
      "2020-11-16 23:51:52,114 : INFO : setting ignored attribute state to None\n",
      "2020-11-16 23:51:52,114 : INFO : setting ignored attribute dispatcher to None\n",
      "2020-11-16 23:51:52,114 : INFO : setting ignored attribute id2word to None\n",
      "2020-11-16 23:51:52,114 : INFO : loaded results_alpenwort_adj\\model-1.lda\n",
      "2020-11-16 23:51:52,114 : INFO : loading LdaState object from results_alpenwort_adj\\model-1.lda.state\n",
      "2020-11-16 23:51:52,224 : INFO : loaded results_alpenwort_adj\\model-1.lda.state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  av_1900_031_TEI.json\n",
      "Total number of entries: 22\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 22\n",
      "File:  av_1900_031_TEI.xml\n",
      "File:  av_1901_032_TEI.json\n",
      "Total number of entries: 19\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 19\n",
      "File:  av_1901_032_TEI.xml\n",
      "File:  av_1902_033_TEI.json\n",
      "Total number of entries: 18\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 18\n",
      "File:  av_1902_033_TEI.xml\n",
      "File:  av_1903_034_TEI.json\n",
      "Total number of entries: 15\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 15\n",
      "File:  av_1903_034_TEI.xml\n",
      "File:  av_1904_035_TEI.json\n",
      "Total number of entries: 20\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 20\n",
      "File:  av_1904_035_TEI.xml\n",
      "File:  av_1905_036_TEI.json\n",
      "Total number of entries: 17\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 17\n",
      "File:  av_1905_036_TEI.xml\n",
      "File:  av_1906_037_TEI.json\n",
      "Total number of entries: 16\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 16\n",
      "File:  av_1906_037_TEI.xml\n",
      "File:  av_1907_038_TEI.json\n",
      "Total number of entries: 15\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 15\n",
      "File:  av_1907_038_TEI.xml\n",
      "File:  av_1908_039_TEI.json\n",
      "Total number of entries: 17\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 17\n",
      "File:  av_1908_039_TEI.xml\n",
      "File:  av_1909_040_TEI.json\n",
      "Total number of entries: 15\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 15\n",
      "File:  av_1909_040_TEI.xml\n",
      "File:  av_1910_041_TEI.json\n",
      "Total number of entries: 13\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 13\n",
      "File:  av_1910_041_TEI.xml\n",
      "File:  av_1911_042_TEI.json\n",
      "Total number of entries: 12\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 12\n",
      "File:  av_1911_042_TEI.xml\n",
      "File:  av_1912_043_TEI.json\n",
      "Total number of entries: 13\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 13\n",
      "File:  av_1912_043_TEI.xml\n",
      "File:  av_1913_044_TEI.json\n",
      "Total number of entries: 14\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 14\n",
      "File:  av_1913_044_TEI.xml\n",
      "File:  av_1914_045_TEI.json\n",
      "Total number of entries: 14\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 14\n",
      "File:  av_1914_045_TEI.xml\n",
      "File:  av_1915_046_TEI.json\n",
      "Total number of entries: 8\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 8\n",
      "File:  av_1915_046_TEI.xml\n",
      "File:  av_1916_047_TEI.json\n",
      "Total number of entries: 12\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 12\n",
      "File:  av_1916_047_TEI.xml\n",
      "File:  av_1917_048_TEI.json\n",
      "Total number of entries: 10\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 10\n",
      "File:  av_1917_048_TEI.xml\n",
      "File:  av_1918_049_TEI.json\n",
      "Total number of entries: 9\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 9\n",
      "File:  av_1918_049_TEI.xml\n",
      "File:  av_1919_050_TEI.json\n",
      "Total number of entries: 4\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 4\n",
      "File:  av_1919_050_TEI.xml\n",
      "File:  av_1920_051_TEI.json\n",
      "Total number of entries: 4\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 4\n",
      "File:  av_1920_051_TEI.xml\n",
      "File:  av_1921_052_TEI.json\n",
      "Total number of entries: 7\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 7\n",
      "File:  av_1921_052_TEI.xml\n",
      "File:  av_1922_053_TEI.json\n",
      "Total number of entries: 6\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 6\n",
      "File:  av_1922_053_TEI.xml\n",
      "File:  av_1923_054_TEI.json\n",
      "Total number of entries: 6\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 6\n",
      "File:  av_1923_054_TEI.xml\n",
      "File:  av_1924_055_TEI.json\n",
      "Total number of entries: 14\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 14\n",
      "File:  av_1924_055_TEI.xml\n",
      "File:  av_1925_056_TEI.json\n",
      "Total number of entries: 10\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 10\n",
      "File:  av_1925_056_TEI.xml\n",
      "File:  av_1926_057_TEI.json\n",
      "Total number of entries: 16\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 16\n",
      "File:  av_1926_057_TEI.xml\n",
      "File:  av_1927_058_TEI.json\n",
      "Total number of entries: 20\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 20\n",
      "File:  av_1927_058_TEI.xml\n",
      "File:  av_1928_059_TEI.json\n",
      "Total number of entries: 13\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 13\n",
      "File:  av_1928_059_TEI.xml\n",
      "File:  av_1929_060_TEI.json\n",
      "Total number of entries: 19\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 19\n",
      "File:  av_1929_060_TEI.xml\n",
      "File:  av_1930_061_TEI.json\n",
      "Total number of entries: 14\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 14\n",
      "File:  av_1930_061_TEI.xml\n",
      "File:  av_1931_062_TEI.json\n",
      "Total number of entries: 18\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 18\n",
      "File:  av_1931_062_TEI.xml\n",
      "File:  av_1932_063_TEI.json\n",
      "Total number of entries: 22\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 22\n",
      "File:  av_1932_063_TEI.xml\n",
      "File:  av_1933_064_TEI.json\n",
      "Total number of entries: 20\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 20\n",
      "File:  av_1933_064_TEI.xml\n",
      "File:  av_1934_065_TEI.json\n",
      "Total number of entries: 19\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 19\n",
      "File:  av_1934_065_TEI.xml\n",
      "File:  av_1935_066_TEI.json\n",
      "Total number of entries: 19\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 19\n",
      "File:  av_1935_066_TEI.xml\n",
      "File:  av_1936_067_TEI.json\n",
      "Total number of entries: 19\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 19\n",
      "File:  av_1936_067_TEI.xml\n",
      "File:  av_1937_068_TEI.json\n",
      "Total number of entries: 23\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 23\n",
      "File:  av_1937_068_TEI.xml\n",
      "File:  av_1938_069_TEI.json\n",
      "Total number of entries: 22\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 22\n",
      "File:  av_1938_069_TEI.xml\n",
      "File:  av_1939_070_TEI.json\n",
      "Total number of entries: 23\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 23\n",
      "File:  av_1939_070_TEI.xml\n",
      "File:  av_1940_071_TEI.json\n",
      "Total number of entries: 28\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 28\n",
      "File:  av_1940_071_TEI.xml\n",
      "File:  av_1941_072_TEI.json\n",
      "Total number of entries: 13\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 13\n",
      "File:  av_1941_072_TEI.xml\n",
      "File:  av_1942_073_TEI.json\n",
      "Total number of entries: 14\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 14\n",
      "File:  av_1942_073_TEI.xml\n",
      "File:  av_1943_000_TEI.json\n",
      "Total number of entries: 0\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 0\n",
      "File:  av_1943_000_TEI.xml\n",
      "File:  av_1949_074_TEI.json\n",
      "Total number of entries: 14\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 14\n",
      "File:  av_1949_074_TEI.xml\n",
      "File:  av_1950_075_TEI.json\n",
      "Total number of entries: 16\n",
      "Removed because of date:  0\n",
      "Removed because too short:  0\n",
      "Remaining: 16\n",
      "File:  av_1950_075_TEI.xml\n",
      "['group', 'url', 'date', 'comment_count', 'words', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49']\n",
      "Created results_alpenwort_adj\\topics-1.json\n"
     ]
    }
   ],
   "source": [
    "# entries published after max_date are ignored\n",
    "utc = pytz.UTC\n",
    "max_date = parser.parse(\"2014-12-31 23:59:59\")\n",
    "max_date_utc = utc.localize(parser.parse(\"2014-12-31 23:59:59\"))\n",
    "\n",
    "number = 0\n",
    "for f in os.listdir(result_path):\n",
    "    try:\n",
    "        number = max(number, int(f.split(model_name+\"-\")[1].split(\".lda\")[0]))\n",
    "    except IndexError:\n",
    "        continue\n",
    "if number > 0:\n",
    "    file_name = model_name + \"-\" + str(number) + \".lda\"\n",
    "else: \n",
    "    file_name = model_name + \".lda\"\n",
    "\n",
    "# load LDA model and dictionary\n",
    "dictionary = corpora.Dictionary.load(os.path.join(result_path, model_name + \".dict\"))\n",
    "model = models.LdaModel.load(os.path.join(result_path, file_name))\n",
    "\n",
    "# new fields for compatibility, default values from\n",
    "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "try:\n",
    "    x = model.minimum_probability\n",
    "except AttributeError:\n",
    "    model.minimum_probability = 0.01\n",
    "    model.minimum_phi_value = 0.01\n",
    "    model.per_word_topics = False\n",
    "    model.random_state = np.random.RandomState()\n",
    "\n",
    "columns = ['group', 'url', 'date', 'comment_count', 'words']\n",
    "columns.extend([str(topic) for topic in range(model.num_topics)])\n",
    "\n",
    "result = []\n",
    "\n",
    "# sort files\n",
    "for json_file in sorted(os.listdir(corpus_path)):\n",
    "\n",
    "    print(\"File: \", json_file)\n",
    "\n",
    "    if json_file.endswith(\".json\"):\n",
    "        # get data\n",
    "        with open(os.path.join(corpus_path, json_file)) as json_data:\n",
    "            data = json.load(json_data)\n",
    "\n",
    "        removed = 0\n",
    "        too_short = 0\n",
    "\n",
    "        for entry in data:\n",
    "            # check if entry is within data range\n",
    "            try:\n",
    "                date = parser.parse(entry[\"date\"])\n",
    "                try:\n",
    "                    if date > max_date:\n",
    "                        removed += 1\n",
    "                        continue\n",
    "                except TypeError:\n",
    "                    if date > max_date_utc:\n",
    "                        removed += 1\n",
    "                        continue\n",
    "            except ValueError:\n",
    "                print(\"Wrong format\", entry[\"date\"])\n",
    "\n",
    "            # get topic distribution for entry\n",
    "            line = {}\n",
    "            try:\n",
    "                text = entry[\"text\"].split(\" \")\n",
    "            except AttributeError:\n",
    "                text = entry[\"text\"]\n",
    "                \n",
    "            # filter too short entries\n",
    "            if len(text) < 5:\n",
    "                too_short += 1\n",
    "                continue\n",
    "\n",
    "            topics = [0] * model.num_topics\n",
    "            for (topic, prop) in model[dictionary.doc2bow(text)]:\n",
    "                topics[topic] = prop\n",
    "            line[\"group\"] = json_file.split(\".json\")[0]\n",
    "            line[\"url\"] = entry[\"url\"]\n",
    "            line[\"date\"] = entry['date']\n",
    "            line[\"words\"] = len(text)\n",
    "            line[\"comment_count\"] = entry[\"comment_count\"]\n",
    "            for counter in range(len(topics)):\n",
    "                line[str(counter)] = topics[counter]\n",
    "            result.append(line)\n",
    "\n",
    "        print(\"Total number of entries:\", len(data))\n",
    "        print(\"Removed because of date: \", removed)\n",
    "        print(\"Removed because too short: \", too_short)\n",
    "        print(\"Remaining:\", (len(data) - removed - too_short))\n",
    "            \n",
    "frame = pd.DataFrame(result)\n",
    "print(columns)\n",
    "frame = frame[columns]\n",
    "start = 1\n",
    "while os.path.isfile(os.path.join(result_path, topics_name + \"-\" +str(start)+ \".json\")):\n",
    "    start += 1\n",
    "\n",
    "frame.to_json(os.path.join(result_path, topics_name + \"-\" + str(start) + \".json\"), orient='split')\n",
    "print (\"Created\", os.path.join(result_path, topics_name + \"-\" + str(start) + \".json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
